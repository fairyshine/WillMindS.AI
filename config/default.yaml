output_total_dir: output/
experiment: main
model_name: LLaMA3_1-8B-Instruct
checkpoint: /PTM/Meta-Llama-3_1-8B-Instruct
seed: 42

tracking:
  type: swanlab

train:
  lr: 5e-4
  epochs: 1
  batch_size: 32
  device: cuda:0
  dtype: bfloat16
  accumulation_steps: 8
  grad_clip: 1.0
  warmup_iters: 0
  log_interval: 100
  save_interval: 100
  local_rank: -1

model:
  dim: 512
  n_layers: 8
  n_heads: 8
  n_kv_heads: 2
  vocab_size: 6400
  hidden_dim: null
  multiple_of: 64
  norm_eps: 1e-5
  max_seq_len: 8192
  rope_theta: 1e6
  dropout: 0.0
  flash_attn: True
  ####################################################
  # Here are the specific configurations of MOE
  # When use_moe is false, the following is invalid
  ####################################################
  use_moe: False
  ####################################################
  num_experts_per_tok: 2
  n_routed_experts: 4
  n_shared_experts: True
  scoring_func: softmax
  aux_loss_alpha: 0.1
  seq_aux: True
  norm_topk_prob: True